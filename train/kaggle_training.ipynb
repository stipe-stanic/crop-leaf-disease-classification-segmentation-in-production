{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install torchsummary"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-01T19:26:43.281873Z",
     "iopub.execute_input": "2023-06-01T19:26:43.282572Z",
     "iopub.status.idle": "2023-06-01T19:26:56.914640Z",
     "shell.execute_reply.started": "2023-06-01T19:26:43.282545Z",
     "shell.execute_reply": "2023-06-01T19:26:56.913393Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchsummary\n",
    "import torchvision.transforms\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets.folder import default_loader, DatasetFolder\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, roc_auc_score, cohen_kappa_score\n",
    "from joblib import dump\n",
    "from PIL import Image\n",
    "from torch import Tensor"
   ],
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "id": "ECQHPsJgzOTN",
    "execution": {
     "iopub.status.busy": "2023-06-01T19:26:56.917382Z",
     "iopub.execute_input": "2023-06-01T19:26:56.917799Z",
     "iopub.status.idle": "2023-06-01T19:27:07.261789Z",
     "shell.execute_reply.started": "2023-06-01T19:26:56.917757Z",
     "shell.execute_reply": "2023-06-01T19:27:07.260760Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "root_dir = '/kaggle/input/crop-diseases/data/plant_dataset_original/plant_diseases_images'\n",
    "model_storage_dir = '/kaggle/working/models_storage'\n",
    "image_dir = '/kaggle/working/images'\n",
    "\n",
    "dir_list = ['best_model_state', 'curr_model_state', 'export_models', 'runs', 'saved_models']\n",
    "\n",
    "if not os.path.exists(model_storage_dir):\n",
    "    os.makedirs(model_storage_dir, exist_ok=True)\n",
    "    \n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    \n",
    "for directory in dir_list:\n",
    "    dir_path = os.path.join(model_storage_dir, directory)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path, exist_ok=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-01T19:27:07.263636Z",
     "iopub.execute_input": "2023-06-01T19:27:07.264451Z",
     "iopub.status.idle": "2023-06-01T19:27:07.273651Z",
     "shell.execute_reply.started": "2023-06-01T19:27:07.264410Z",
     "shell.execute_reply": "2023-06-01T19:27:07.272506Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom ImageFolder class\n",
    "\n",
    "def valid_file(filename: str) -> bool:\n",
    "    \"\"\"Check if current file has valid extension\"\"\"\n",
    "\n",
    "    return filename.lower().endswith(('.jpg', '.png'))\n",
    "\n",
    "\n",
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folders in a dataset.\n",
    "\n",
    "    This function searches for subdirectories in the specified directory and returns a list of class names and\n",
    "    dictionary mapping each class name to its corresponding index.\n",
    "\n",
    "    The function only includes subdirectories whose names match the regular expression 'apple'. This is intended to\n",
    "    filter the classes for a specific type of dataset.\n",
    "\n",
    "    :param directory: The root directory of the training dataset.\n",
    "\n",
    "    :returns: A tuple containing a list of strings where each string is the name of a class folder and dictionary\n",
    "    that maps each class name to its corresponding index.\n",
    "\n",
    "    :raises FileNotFoundError: If no class folders are found in the specified directory.\n",
    "    \"\"\"\n",
    "\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "class CustomImageFolder(torchvision.datasets.DatasetFolder):\n",
    "    \"\"\" Implements custom ImageFolder class that overrides DatasetFolder methods, so it's possible to load only\n",
    "    specific subdirectories(classes) of the directory instead of the whole directory.\n",
    "\n",
    "    Enables two valid extensions (.jpg, .png)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, target_transform=None, loader=default_loader, is_valid_file=valid_file):\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform, loader=loader, is_valid_file=is_valid_file)\n",
    "    def find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "        return find_classes(directory)"
   ],
   "metadata": {
    "id": "0G_DNBG9zOTP",
    "execution": {
     "iopub.status.busy": "2023-06-01T19:27:07.277217Z",
     "iopub.execute_input": "2023-06-01T19:27:07.277610Z",
     "iopub.status.idle": "2023-06-01T19:27:07.288961Z",
     "shell.execute_reply.started": "2023-06-01T19:27:07.277576Z",
     "shell.execute_reply": "2023-06-01T19:27:07.287501Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualization\n",
    "\n",
    "def show_dataset(dataset: DatasetFolder | Subset, num_called=0, n=6) -> None:\n",
    "    \"\"\"Show grid of images as a single image\n",
    "\n",
    "    :param dataset: Loaded torchvision dataset\n",
    "    :param n: Number of rows and columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Transform image from tensor to PILImage\n",
    "    transform = torchvision.transforms.ToPILImage()\n",
    "    img = np.vstack([np.hstack([np.asarray(transform(dataset[i][0])) for _ in range(n)])\n",
    "                     for i in range(n)])\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if num_called == 1:\n",
    "        plt.savefig(image_dir + '/dataset_transform.png')\n",
    "    elif num_called == 2:\n",
    "        plt.savefig(image_dir + '/train_dataset_transform.png')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def show_batch(dataset_loader: DataLoader, num_of_images: int = 9) -> None:\n",
    "    \"\"\"Display images before feeding them to the model\n",
    "\n",
    "    :raises AssertionError: If number of images to display exceeds batch size\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = dataset_loader.batch_size\n",
    "\n",
    "    try:\n",
    "        assert num_of_images < batch_size,\\\n",
    "            f\"Number of images to display exceeds batch size: {num_of_images} > {batch_size}\"\n",
    "\n",
    "        data_iter = iter(dataset_loader)\n",
    "        images, labels = next(data_iter)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        transform = torchvision.transforms.ToPILImage()\n",
    "        for i in range(num_of_images):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            img = np.asarray(transform(images[i]))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.savefig(image_dir + '/train_loader.png')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    except AssertionError as msg:\n",
    "        print(\"Error:\", msg)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, class_names: List[str]) -> None:\n",
    "    \"\"\"Plot the confusion matrix using a heatmap.\n",
    "\n",
    "   :param y_true: True labels of the test set.\n",
    "   :param y_pred: Predicted labels of the test set.\n",
    "   :param class_names: List of class names.\n",
    "   \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=class_names,\n",
    "                                            normalize='true', xticks_rotation=\"vertical\",\n",
    "    \n",
    "                                            ax=ax, colorbar=False)\n",
    "    plt.savefig(image_dir + '/confusion_matrix.png')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "\n",
    "        \n",
    "def plot_classification_report(y_true: np.ndarray, y_pred: np.ndarray, class_names: List[str]) -> None:\n",
    "    \"\"\"Plot a heatmap visualization of a classification report.\n",
    "\n",
    "    :param y_true: True labels of the test set .\n",
    "    :param y_pred: Predicted labels of the test set.\n",
    "    :param class_names: A list of class names corresponding to the labels.\n",
    "    \"\"\"\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df = df.drop(['support'], axis=1)\n",
    "    sns.heatmap(df, annot=True, cmap='YlGnBu', fmt='.2f')\n",
    "    plt.title('Classification Report Heatmap')\n",
    "    plt.savefig(image_dir + '/classification_report_heatmap.png')\n",
    "    plt.show(block=False)"
   ],
   "metadata": {
    "id": "-tj08VW9zOTQ",
    "execution": {
     "iopub.status.busy": "2023-06-01T19:27:07.290154Z",
     "iopub.execute_input": "2023-06-01T19:27:07.290668Z",
     "iopub.status.idle": "2023-06-01T19:27:07.308268Z",
     "shell.execute_reply.started": "2023-06-01T19:27:07.290633Z",
     "shell.execute_reply": "2023-06-01T19:27:07.307306Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Util functions\n",
    "\n",
    "def loader_shape(dataset_loader: DataLoader) -> Tuple[torch.Size, torch.Size]:\n",
    "    \"\"\"Print shape of loaded dataset\n",
    "\n",
    "    :returns: Tuple of tensor shapes (images, labels)\n",
    "    \"\"\"\n",
    "\n",
    "    data_iter = iter(dataset_loader)\n",
    "    images, labels = next(data_iter)\n",
    "\n",
    "    return images.shape, labels.shape\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Get the device for running PyTorch computations.\n",
    "\n",
    "    :returns: torch.device: The selected device (CPU or GPU).\n",
    "    \"\"\"\n",
    "\n",
    "    cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "def stratify_split(dataset: CustomImageFolder) -> Tuple[Subset, Subset, Subset]:\n",
    "    \"\"\"Split a dataset into training, validation, and test subsets while preserving class distribution.\n",
    "\n",
    "    :param: dataset: The dataset to be split.\n",
    "    :returns: The training, validation, and test subsets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the dataset the to ratio of 80/20\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    val_size = int(len(dataset) * 0.1)\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    # Converts the dataset targets to a numpy array\n",
    "    targets = np.array(dataset.targets)\n",
    "\n",
    "    # Splits the indices into train and temp sets, stratifying based on targets\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        np.arange(len(dataset)),\n",
    "        test_size=val_size + test_size,\n",
    "        shuffle=True,\n",
    "        stratify=targets,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    # Split the temp indices into validation and test sets\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx,\n",
    "        test_size=test_size,\n",
    "        shuffle=True,\n",
    "        stratify=targets[temp_idx],\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset = Subset(dataset, val_idx)\n",
    "    test_dataset = Subset(dataset, test_idx)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "def custom_clahe_transform(img: Image) -> Image:\n",
    "    \"\"\"Apply custom Contrast Limited Adaptive Histogram Equalization (CLAHE) transformation to an image.\n",
    "\n",
    "    :param img: The input image to be transformed.\n",
    "    :returns: The transformed image.\n",
    "    \"\"\"\n",
    "\n",
    "    transform = CustomCLAHE(clip_limit=2.0, tile_grid_size=(8, 8))\n",
    "    return transform(img)"
   ],
   "metadata": {
    "id": "odhBcFF2zOTQ",
    "execution": {
     "iopub.status.busy": "2023-06-01T19:27:07.309702Z",
     "iopub.execute_input": "2023-06-01T19:27:07.310175Z",
     "iopub.status.idle": "2023-06-01T19:27:07.323165Z",
     "shell.execute_reply.started": "2023-06-01T19:27:07.310129Z",
     "shell.execute_reply": "2023-06-01T19:27:07.322067Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Util classes\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha: float = 0.25, gamma: float = 2.0, num_classes=4):\n",
    "        \"\"\"Focal loss for multiclass classification.\n",
    "\n",
    "        :param alpha: Weighting factor, positive class samples are given four times less weight than negative class samples.\n",
    "        :param gamma: Focusing parameter, gamma > 1 increases emphasis on correctly classifying challenging scenarios.\n",
    "        :param num_classes: Number of classes.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, cls_preds: Tensor, cls_targets: Tensor) -> Tensor:\n",
    "        \"\"\"Compute focal loss between cls_preds and cls_targets\n",
    "\n",
    "        :param cls_preds: predicted class probabilities, sized [batch_size, num_classes]\n",
    "        :param cls_targets: target class labels, sized [batch_size, num_classes]\n",
    "\n",
    "        :returns: focal loss\n",
    "        \"\"\"\n",
    "\n",
    "        # Converts target labels to one-hot encoded values\n",
    "        t = F.one_hot(cls_targets, self.num_classes).float()  # [batch_size, num_classes]\n",
    "        t = t.to(cls_preds.device)\n",
    "\n",
    "        # Small value to avoid numerical instability\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        p = torch.clamp(cls_preds.softmax(dim=1), min=epsilon, max=1 - epsilon)\n",
    "\n",
    "        # Compute focal loss\n",
    "        focal_loss = -self.alpha * (t * torch.log(p + epsilon) + (1 - t) * torch.log(1 - p + epsilon))\n",
    "        focal_loss = focal_loss.sum()\n",
    "\n",
    "        loss = focal_loss.mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class CustomCLAHE(object):\n",
    "    def __init__(self, clip_limit: float = 2.0, tile_grid_size: Tuple[int, int] = (8, 8)):\n",
    "        \"\"\"Initializes an object of CustomCLAHE class with specified parameters.\n",
    "\n",
    "        :param clip_limit: The contrast limit.\n",
    "        :param tile_grid_size: The size of the grid.\n",
    "        \"\"\"\n",
    "\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "\n",
    "    def __call__(self, img: Image) -> Image:\n",
    "        \"\"\"Applies Contrast Limited Adaptive Histogram Equalization to the input image.\n",
    "\n",
    "        :param img: The input image.\n",
    "        :returns: The equalized image.\n",
    "        \"\"\"\n",
    "\n",
    "        img = np.array(img)\n",
    "\n",
    "        # Converts the image from BGR to LAB color space\n",
    "        lab_image = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        l_channel, a_channel, b_channel = cv2.split(lab_image)\n",
    "\n",
    "        # Applies CLAHE to L channel\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        equalized_l_channel = clahe.apply(l_channel)\n",
    "\n",
    "        # Merges the equalized L channel with original a and b channels\n",
    "        equalized_lab_image = cv2.merge([equalized_l_channel, a_channel, b_channel])\n",
    "\n",
    "        # Converts the equalized LAB image back to BGR color space\n",
    "        equalized_bgr_image = cv2.cvtColor(equalized_lab_image, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "        return Image.fromarray(equalized_bgr_image)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-01T19:27:07.324527Z",
     "iopub.execute_input": "2023-06-01T19:27:07.325122Z",
     "iopub.status.idle": "2023-06-01T19:27:07.342262Z",
     "shell.execute_reply.started": "2023-06-01T19:27:07.325052Z",
     "shell.execute_reply": "2023-06-01T19:27:07.341244Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# LR_ASK\n",
    "\n",
    "class LR_ASK:\n",
    "    def __init__(self, model: nn.Module, optimizer: optim.Optimizer, epochs: int, ask_epoch: int):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.ask_epoch = ask_epoch\n",
    "        self.ask = True\n",
    "        self.lowest_vloss = float('inf')\n",
    "        self.best_weights = model.state_dict()\n",
    "        self.best_epoch = 1\n",
    "        self.start_time = None\n",
    "\n",
    "    def on_train_begin(self) -> None:\n",
    "        \"\"\"Method called at the beginning of the training process, checks the ask_epoch and epochs values to determine\n",
    "        the behavior of training\n",
    "        \"\"\"\n",
    "\n",
    "        if self.ask_epoch == 0:\n",
    "            print('You set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n",
    "            self.ask_epoch = 1\n",
    "\n",
    "        if self.ask_epoch >= self.epochs:\n",
    "            print('Ask_epoch >= epochs, will train for', self.epochs, 'epochs', flush=True)\n",
    "            self.ask = False\n",
    "        elif self.epochs == 1:\n",
    "            self.ask = False\n",
    "        else:\n",
    "            print('Training will proceed until epoch', self.ask_epoch,\n",
    "                  'then you will be asked to enter H(h) to halt training or enter an integer'\n",
    "                  ' to continue training for n number of epochs')\n",
    "\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        \"\"\"Called at the end of the training process, loads the weights of the model with the lowest\n",
    "        validation loss\n",
    "        \"\"\"\n",
    "\n",
    "        print('Loading model with weights from epoch', self.best_epoch)\n",
    "        self.model.load_state_dict(self.best_weights)\n",
    "\n",
    "        train_duration = time.time() - self.start_time\n",
    "        hours = int(train_duration // 3600)\n",
    "        minutes = int((train_duration % 3600) // 60)\n",
    "        seconds = train_duration % 60\n",
    "\n",
    "        msg = f'Training time: {hours:02d}h:{minutes:02d}m:{seconds:02.0f}s'\n",
    "        print(msg, flush=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, val_loss: Tensor):\n",
    "        \"\"\"Called at the end of each training epoch, receives the current epoch number\n",
    "        and the validation loss tensor. Saves the best weights if v_loss is lower.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extracts the scalar value from validation loss tensor\n",
    "        v_loss = val_loss.item()\n",
    "        if v_loss < self.lowest_vloss:\n",
    "            self.lowest_vloss = v_loss\n",
    "            self.best_weights = self.model.state_dict()\n",
    "            self.best_epoch = epoch + 1\n",
    "\n",
    "            print(f'\\nValidation loss of {v_loss:.4f} is below lowest loss,'\n",
    "                  f' saving weights from epoch {str(epoch + 1)} as best weights')\n",
    "        else:\n",
    "            print(f'\\nValidation loss of {v_loss:.4f} is above lowest loss of {self.lowest_vloss:.4f}'\n",
    "                  f' keeping weights from epoch {str(self.best_epoch)} as best weights')\n",
    "\n",
    "        if self.ask and epoch + 1 == self.ask_epoch:\n",
    "            print('\\nEnter H(h) to end training or enter an integer for the number of additional epochs to run')\n",
    "            ans = input()\n",
    "\n",
    "            if ans == 'H' or ans == 'h':\n",
    "                print(f'You entered {ans}, training halted on epoch {epoch + 1}, due to user input\\n', flush=True)\n",
    "                raise KeyboardInterrupt\n",
    "            else:\n",
    "                self.ask_epoch += int(ans)\n",
    "                if self.ask_epoch > self.epochs:\n",
    "                    print('\\nYou specified maximum epochs as', self.epochs,\n",
    "                          'cannot train for', self.ask_epoch, flush=True)\n",
    "                else:\n",
    "                    print(f'You entered {ans}, training will continue to epoch {self.ask_epoch}', flush=True)\n",
    "\n",
    "                    lr = self.optimizer.param_groups[0]['lr']\n",
    "                    print(f'Current LR is {lr}, enter C(c) to keep this LR or enter a float number for a new LR')\n",
    "\n",
    "                    ans = input()\n",
    "                    if ans == 'C' or ans == 'c':\n",
    "                        print(f'Keeping current LR of {lr}\\n')\n",
    "                    else:\n",
    "                        new_lr = float(ans)\n",
    "                        for param_group in self.optimizer.param_groups:\n",
    "                            param_group['lr'] = new_lr\n",
    "                        print('Changing LR to\\n', ans)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-01T19:27:07.346042Z",
     "iopub.execute_input": "2023-06-01T19:27:07.346393Z",
     "iopub.status.idle": "2023-06-01T19:27:07.364634Z",
     "shell.execute_reply.started": "2023-06-01T19:27:07.346367Z",
     "shell.execute_reply": "2023-06-01T19:27:07.363495Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with two 3x3 convolutional layers followed by batch normalization layers and max pooling.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def conv_block(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the convolutional block to the input tensor x\"\"\"\n",
    "\n",
    "        return self.conv_block(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with two convolutions followed by batch normalization layers\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, hidden_channels: int,  out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def residual_block(self, x):\n",
    "        x1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        x2 = F.relu(self.bn2(self.conv2(x1)))\n",
    "        return x2 + x\n",
    "\n",
    "    def forward(self, x): return self.residual_block(x)\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, after_conv: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(in_channels * 7 * 7, out_channels) if after_conv else nn.Linear(in_channels, out_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def linear_block(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x): return self.linear_block(x)\n",
    "\n",
    "\n",
    "class ResModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResModel, self).__init__()\n",
    "\n",
    "        self.conv1 = ConvBlock(3, 32, 32)\n",
    "        self.res1 = ResidualBlock(32, 32, 32)\n",
    "\n",
    "        self.conv2 = ConvBlock(32, 64, 64)\n",
    "        self.res2 = ResidualBlock(64, 64, 64)\n",
    "\n",
    "        self.conv3 = ConvBlock(64, 128, 128)\n",
    "        self.res3 = ResidualBlock(128, 128, 128)\n",
    "\n",
    "        self.conv4 = ConvBlock(128, 256, 256)\n",
    "        self.res4 = ResidualBlock(256, 256, 256)\n",
    "\n",
    "        self.conv5 = ConvBlock(256, 512, 512)\n",
    "        self.res5 = ResidualBlock(512, 512, 512)\n",
    "\n",
    "        self.fc1 = LinearBlock(512, 1024, after_conv=True)\n",
    "        self.fc2 = LinearBlock(1024, 1024)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1024, 32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Conv blocks and residual blocks\n",
    "        x = self.conv1(x)\n",
    "        x = self.res1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.res2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.res3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.res4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.res5(x)\n",
    "\n",
    "        # Linear blocks\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n"
   ],
   "metadata": {
    "id": "ySjINFZ67LjR",
    "execution": {
     "iopub.status.busy": "2023-06-01T19:27:07.366211Z",
     "iopub.execute_input": "2023-06-01T19:27:07.366914Z",
     "iopub.status.idle": "2023-06-01T19:27:07.390957Z",
     "shell.execute_reply.started": "2023-06-01T19:27:07.366880Z",
     "shell.execute_reply": "2023-06-01T19:27:07.390130Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Config\n",
    "batch_size = 32\n",
    "epochs = 22\n",
    "seed = 255\n",
    "\n",
    "focal_loss = {\n",
    "    'alpha': 0.50,\n",
    "    'gamma': 1.75\n",
    "}\n",
    "\n",
    "adamax_lr = 0.001\n",
    "adamax_weight_decay = 0.001\n",
    "\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "id": "O1no3eWTzOTR",
    "execution": {
     "iopub.status.busy": "2023-06-01T19:27:07.395857Z",
     "iopub.execute_input": "2023-06-01T19:27:07.396579Z",
     "iopub.status.idle": "2023-06-01T19:27:07.411446Z",
     "shell.execute_reply.started": "2023-06-01T19:27:07.396542Z",
     "shell.execute_reply": "2023-06-01T19:27:07.409803Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Preprocessing\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "writer = SummaryWriter(model_storage_dir + '/runs/model_01')\n",
    "\n",
    "dataset_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.46445759, 0.49094302, 0.41258632), (0.1741543, 0.14767326, 0.19304359))\n",
    "])\n",
    "\n",
    "dump(dataset_transforms, model_storage_dir + '/saved_models/transform.joblib', compress=True)\n",
    "\n",
    "dataset = CustomImageFolder(root=root_dir, loader=default_loader, transform=dataset_transforms)\n",
    "\n",
    "num_called_show_dataset = 1\n",
    "show_dataset(dataset, num_called_show_dataset)\n",
    "\n",
    "class_names = dataset.classes\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(dataset)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "\n",
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Lambda(custom_clahe_transform),  # increases contrast in a smart way\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.RandomRotation(20, interpolation=torchvision.transforms.InterpolationMode.BILINEAR,\n",
    "                                          expand=False),\n",
    "    torchvision.transforms.ColorJitter(brightness=.05, contrast=0.5, saturation=.05, hue=.05),\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset.dataset.transform = train_transforms\n",
    "\n",
    "num_called_show_dataset += 1\n",
    "show_dataset(train_dataset, num_called_show_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                           num_workers=2, pin_memory=True,\n",
    "                                           generator=torch.Generator().manual_seed(seed))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                         num_workers=2, pin_memory=True,\n",
    "                                         generator=torch.Generator().manual_seed(seed))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                          num_workers=2, pin_memory=True,\n",
    "                                          generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "images_shape, labels_shape = loader_shape(train_loader)\n",
    "print(f'Images shape: {images_shape}\\nLabels shape: {labels_shape}')\n",
    "\n",
    "show_batch(train_loader)"
   ],
   "metadata": {
    "id": "zDsmslg8zOTR",
    "outputId": "cf40cb6a-d903-4ed2-9027-b61d166b3f8d",
    "execution": {
     "iopub.status.busy": "2023-06-01T19:27:07.413275Z",
     "iopub.execute_input": "2023-06-01T19:27:07.413925Z",
     "iopub.status.idle": "2023-06-01T19:28:30.957038Z",
     "shell.execute_reply.started": "2023-06-01T19:27:07.413892Z",
     "shell.execute_reply": "2023-06-01T19:28:30.955924Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Logging\n",
    "\n",
    "log_file_path = '/kaggle/working/log.txt'\n",
    "log_file = open(log_file_path, 'w')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-01T19:28:30.960976Z",
     "iopub.execute_input": "2023-06-01T19:28:30.961788Z",
     "iopub.status.idle": "2023-06-01T19:28:30.969323Z",
     "shell.execute_reply.started": "2023-06-01T19:28:30.961759Z",
     "shell.execute_reply": "2023-06-01T19:28:30.968369Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train\n",
    "\n",
    "model = ResModel().to(device)\n",
    "# torchsummary.summary(model, (3, 224, 224), batch_size=batch_size)\n",
    "\n",
    "# loss_fn = nn.NLLLoss().to(device)\n",
    "loss_fn = FocalLoss(alpha=focal_loss['alpha'], gamma=focal_loss['gamma'],\n",
    "                    num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = optim.Adamax(model.parameters(), lr=adamax_lr, weight_decay=adamax_weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=3)\n",
    "\n",
    "lr_ask_callback = LR_ASK(model, optimizer, epochs, ask_epoch=24)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "loss = None\n",
    "\n",
    "train_per_epoch = int(len(train_dataset) / batch_size)\n",
    "lr_ask_callback.on_train_begin()\n",
    "for e in range(epochs):\n",
    "    print(f'Learning rate: {scheduler.get_last_lr()[0]}')\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "    model.train()\n",
    "\n",
    "    train_acc = 0\n",
    "    train_losses = []\n",
    "    for idx, (images, labels) in loop:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar('loss', loss.item(), (e * train_per_epoch) + idx)\n",
    "\n",
    "        # squeeze() removes singleton dimensions, resulting in a 1D tensor representing\n",
    "        # the predicted class labels for each input sample\n",
    "        predictions = output.argmax(dim=1, keepdims=True).squeeze()\n",
    "\n",
    "        correct = (predictions == labels).sum().item()\n",
    "        accuracy = correct / len(predictions)\n",
    "\n",
    "        loop.set_description(f\"Epoch [{e + 1}/{epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item(), acc=accuracy, refresh=False)\n",
    "        writer.add_scalar('acc', accuracy, (e * train_per_epoch) + idx)\n",
    "\n",
    "        train_acc += correct\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "    else:\n",
    "        torch.save({\n",
    "            'epoch': e,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, model_storage_dir + '/curr_model_state/last_train_model_state.pth')\n",
    "\n",
    "    train_acc /= len(train_dataset)\n",
    "    train_loss = np.array(train_losses).mean()\n",
    "    \n",
    "    print_train_metric = f'Epoch [{e + 1}/{epochs}]: Train accuracy = {train_acc:.4f} Train loss: {train_loss:.4f}'\n",
    "    log_file.write(print_train_metric)\n",
    "    log_file.flush()\n",
    "    print(print_train_metric)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    val_acc = 0.0\n",
    "    val_losses = []\n",
    "\n",
    "    # Disables dropout layers and batch normalization layers use population statistics for normalization\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            val_loss = loss_fn(scores, y)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            # max() function computes maximum value and its corresponding index along dim=1 axis. Returns tensor\n",
    "            # containing the maximum values and tensor containing the indices of maximum values\n",
    "            _, predictions = scores.max(1)\n",
    "            val_acc += (predictions == y).sum().item()\n",
    "\n",
    "    val_acc /= len(val_dataset)\n",
    "    val_loss = np.array(val_losses).mean()\n",
    "    \n",
    "    print_val_metric = f'Epoch [{e + 1}/{epochs}]: Validation accuracy = {val_acc:.4f} Validation loss: {val_loss:.4f}'\n",
    "    log_file.write(print_val_metric)\n",
    "    log_file.flush()\n",
    "    print(print_val_metric)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': e,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "        }, model_storage_dir + '/curr_model_state/best_val_loss_checkpoint.pth')\n",
    "\n",
    "    try:\n",
    "        lr_ask_callback.on_epoch_end(e, val_loss)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "lr_ask_callback.on_train_end()\n",
    "\n",
    "log_file.close()\n",
    "\n",
    "torch.save(model.state_dict(), model_storage_dir + '/saved_models/ResModel.pth')\n",
    "\n",
    "zip_file_path = '/kaggle/working/output'\n",
    "\n",
    "shutil.make_archive(zip_file_path, 'zip', '/kaggle/working/')"
   ],
   "metadata": {
    "id": "sbpegjkQzOTS",
    "outputId": "a3073142-9356-42d4-e780-ccda1267a285",
    "execution": {
     "iopub.status.busy": "2023-06-01T19:28:30.972737Z",
     "iopub.execute_input": "2023-06-01T19:28:30.973020Z",
     "iopub.status.idle": "2023-06-01T19:48:41.342348Z",
     "shell.execute_reply.started": "2023-06-01T19:28:30.972993Z",
     "shell.execute_reply": "2023-06-01T19:48:41.341055Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": "Training will proceed until epoch 1 then you will be asked to enter H(h) to halt training or enter an integer to continue training for n number of epochs\nLearning rate: 0.001\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Epoch [1/20]: 100%|██████████| 2389/2389 [16:44<00:00,  2.38it/s, acc=0.5, loss=9.67]  \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch [1/20]: Train accuracy = 0.3913 Train loss: 46.2878\nEpoch [1/20]: Validation accuracy = 0.5562 Validation loss: 34.8569\n\nValidation loss of 34.8569 is below lowest loss, saving weights from epoch 1 as best weights\n\nEnter H(h) to end training or enter an integer for the number of additional epochs to run\n",
     "output_type": "stream"
    },
    {
     "output_type": "stream",
     "name": "stdin",
     "text": " h\n"
    },
    {
     "name": "stdout",
     "text": "You entered h, training halted on epoch 1, due to user input\n\nLoading model with weights from epoch 1\nTraining time: 00h:19m:09s\n",
     "output_type": "stream"
    },
    {
     "execution_count": 13,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'/kaggle/working/output.zip'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Test\n",
    "\n",
    "num_correct = 0\n",
    "num_samples = 0\n",
    "y_true, y_pred, y_score = [], [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:  # images, labels\n",
    "        x = x.to(device=device)\n",
    "        y = y.to(device=device)\n",
    "\n",
    "        scores = model(x)\n",
    "        _, predictions = scores.max(1)\n",
    "\n",
    "        num_correct += (predictions == y).sum()\n",
    "        num_samples += predictions.size(0)\n",
    "\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "        y_score.extend(scores.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_score = np.array(y_score)\n",
    "\n",
    "    # axis=0: column-wise sum, axis=1: row-wise sum\n",
    "    y_prob = np.exp(y_score) / np.sum(np.exp(y_score), axis=1, keepdims=True)\n",
    "\n",
    "    print(f'Number of correct {num_correct} of total {num_samples} with accuracy of'\n",
    "          f' {float(num_correct) / float(num_samples) * 100:.2f}%\\n')\n",
    "\n",
    "    # Measures the model's ability to distinguish between the positive and negative classes,\n",
    "    # 'ovr' treats one class as positive and the remaining classes as negative\n",
    "    print(f'Area under the ROC curve: {roc_auc_score(y_true, y_prob, multi_class=\"ovr\")}')\n",
    "\n",
    "    # Provides a measure of agreement that is adjusted for the possibility of agreement occurring by chance\n",
    "    print(f'Cohen kappa score: {cohen_kappa_score(y_true, y_pred)}')"
   ],
   "metadata": {
    "id": "zuyZVSAOzOTS",
    "execution": {
     "iopub.status.busy": "2023-06-01T19:16:55.178020Z",
     "iopub.execute_input": "2023-06-01T19:16:55.178799Z",
     "iopub.status.idle": "2023-06-01T19:18:57.488756Z",
     "shell.execute_reply.started": "2023-06-01T19:16:55.178761Z",
     "shell.execute_reply": "2023-06-01T19:18:57.487567Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Analysis\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, class_names)\n",
    "plot_classification_report(y_true, y_pred, class_names)"
   ],
   "metadata": {
    "id": "abQ2rOcBzOTS",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
